# Default values for vllm chart.

# global:
#   suspend: false

llm-serving:
  nameOverride: ""
  fullnameOverride: "llm-serving"
  podAnnotations: {}

  service:
    image:
      version: "vllm/vllm-openai:v0.7.0"
      pullPolicy: IfNotPresent
      custom:
        enabled: false
        version: ""

  resources: {}

  huggingFace: 
    hfToken: ""

  llm:
    hfNamespace: meta-llama
    hfModel: Llama-3.2-1B-Instruct
    localPath: "/root/.cache/huggingface"
    memoryUtilization: 0.9
    dtype: "auto"
    maxModelLen": 8200
    
  networking:
    port:
      number: 8000

  ingress:
      enabled: false

  s3:
    enabled: true # Set to true to use S3
    modelHfBucket: ""
    accessKeyId: ""
    endpoint: ""
    defaultRegion: ""
    secretAccessKey: ""
    sessionToken: ""

  nodeSelector: {}

ingress:
  enabled: true
  tls: true
  ingressClassName: ""
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
  hostname: chart-example.local
  useCertManager: false
  certManagerClusterIssuer: ""

networking:
  service:
    port: 8000

security:
  allowlist:
    enable: false
