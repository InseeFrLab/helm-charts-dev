# Default values for vllm chart.

# global:
#   suspend: false

llm-serving:
  nameOverride: ""
  fullnameOverride: "llm-serving"
  podAnnotations: {}

  service:
    image:
      version: "vllm/vllm-openai:v0.7.0"
      pullPolicy: IfNotPresent
      custom:
        enabled: false
        version: ""

  resources: {}

  huggingFace: 
    hfToken: ""

  llm:
    hfNamespace: meta-llama
    hfModel: Llama-3.2-1B-Instruct
    localPath: "/root/.cache/huggingface"
    memoryUtilization: 0.8
    dtype: "half"
    maxModelLen": 8208
    
  networking:
    port:
      number: 8000

  ingress:
      enabled: false

  s3:
    enabled: false # Set to true to use S3
    bucket: "your_bucket_name"
    modelPath: "path_to_model"
    # If not set and create is true, a name is generated using the fullname template
    accessKeyId: ""
    endpoint: ""
    defaultRegion: ""
    secretAccessKey: ""
    sessionToken: ""

ingress:
  enabled: true
  tls: true
  ingressClassName: ""
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
  hostname: chart-example.local
  useCertManager: false
  certManagerClusterIssuer: ""
